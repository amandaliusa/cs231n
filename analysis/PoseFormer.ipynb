{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import git\n",
    "import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "repo = git.Repo(\"./\", search_parent_directories=True)\n",
    "homedir = repo.working_dir\n",
    "dtype=torch.float32\n",
    "\n",
    "from vis_poseformer import *\n",
    "\n",
    "USE_GPU = True\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load precomputed keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[171.34386, 256.31653],\n",
       "         [158.9295 , 260.97192],\n",
       "         [196.17256, 307.52573],\n",
       "         ...,\n",
       "         [140.30797, 177.17503],\n",
       "         [177.55103, 208.21092],\n",
       "         [202.37973, 183.38222]],\n",
       "\n",
       "        [[169.83136, 256.83423],\n",
       "         [155.99959, 262.98166],\n",
       "         [195.95804, 306.01385],\n",
       "         ...,\n",
       "         [140.63097, 176.91733],\n",
       "         [177.51569, 207.65459],\n",
       "         [202.1055 , 183.06477]],\n",
       "\n",
       "        [[170.2119 , 256.56805],\n",
       "         [156.36711, 262.72128],\n",
       "         [196.36317, 305.79398],\n",
       "         ...,\n",
       "         [140.98401, 176.57591],\n",
       "         [177.90346, 207.34212],\n",
       "         [199.43979, 182.72916]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[174.77771, 269.71793],\n",
       "         [161.14906, 274.2608 ],\n",
       "         [200.5207 , 319.68964],\n",
       "         ...,\n",
       "         [164.17766, 195.51753],\n",
       "         [194.46353, 225.8034 ],\n",
       "         [227.778  , 201.5747 ]],\n",
       "\n",
       "        [[174.68729, 269.54486],\n",
       "         [161.17844, 274.0478 ],\n",
       "         [200.20403, 322.07928],\n",
       "         ...,\n",
       "         [164.1804 , 195.99663],\n",
       "         [194.20009, 226.01631],\n",
       "         [227.22173, 202.00056]],\n",
       "\n",
       "        [[174.26666, 267.08917],\n",
       "         [160.8375 , 273.05768],\n",
       "         [199.63287, 320.80585],\n",
       "         ...,\n",
       "         [160.8375 , 192.48268],\n",
       "         [190.6801 , 225.30954],\n",
       "         [220.52269, 201.43547]]]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2d keypoints\n",
    "keypoints = np.load(\"{}/pretrained_models/poseformer/1ovgasC1/input_2D/keypoints.npz\".format(homedir), allow_pickle=True)\n",
    "keypoints['reconstruction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 325, 17, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypoints['reconstruction'].shape\n",
    "# (, frames, keypoints, dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(325, 17, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3d output (frames, keypoints, dimensions)\n",
    "output_3d = np.load(\"{}/pretrained_models/poseformer/1ovgasC1/3d_output.npy\".format(homedir), allow_pickle=True)\n",
    "output_3d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 354, 17, 2)\n",
      "(354, 17, 3)\n"
     ]
    }
   ],
   "source": [
    "keypoints = np.load(\"{}/pretrained_models/poseformer/AnEq8Uph/input_2D/keypoints.npz\".format(homedir), allow_pickle=True)\n",
    "print(keypoints['reconstruction'].shape) \n",
    "output_3d = np.load(\"{}/pretrained_models/poseformer/AnEq8Uph/3d_output.npy\".format(homedir), allow_pickle=True)\n",
    "print(output_3d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate demo \n",
    "\n",
    "Using precomputed keypoints, generate images and video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1ovgasC1\n"
     ]
    }
   ],
   "source": [
    "video_path = '{}/pretrained_models/poseformer/video/1ovgasC1.mp4'.format(homedir)\n",
    "video_name = \".\".join(video_path.split('/')[-1].split('.')[:-1])\n",
    "print(video_name)\n",
    "output_dir = '{}/pretrained_models/poseformer/1ovgasC1/'.format(homedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/325 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating 3D pose...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 20/325 [00:06<02:08,  2.37it/s]C:\\Users\\amanda\\Documents\\GitHub\\cs231n\\analysis\\vis_poseformer.py:239: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = plt.figure( figsize=(9.6, 5.4))\n",
      "100%|██████████| 325/325 [01:16<00:00,  4.24it/s]\n",
      "  0%|          | 0/325 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 3D pose successful!\n",
      "\n",
      "Generating demo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 325/325 [03:58<00:00,  1.36it/s]\n"
     ]
    }
   ],
   "source": [
    "get_pose3D(video_path, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2video(video_path, output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
